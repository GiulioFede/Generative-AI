{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xzn0YrUaaWjx",
        "cuc9qvyWWHUf",
        "-Q97vaC3NlqR",
        "TrqW_O67as26",
        "7iZsvZOfgOgW",
        "u_P94_AwVz8o"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxK034M6v7X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.datasets import load_digits\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.multivariate_normal import MultivariateNormal\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib as matplotlib\n",
        "print(pd.__version__)\n",
        "print(np.__version__)\n",
        "print(matplotlib.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYiLt9jPIlzB",
        "outputId": "a7c9dfd9-2cb6-4eb0-c82b-6474dc53575c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.5.3\n",
            "1.22.4\n",
            "3.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Controlla la disponibilità della GPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")  # Imposta il dispositivo sulla GPU\n",
        "else:\n",
        "    device = torch.device(\"cpu\")  # Se la GPU non è disponibile, utilizza la CPU\n",
        "\n",
        "print(device)"
      ],
      "metadata": {
        "id": "JbQBUpStOG34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde9c588-8c34-4793-a514-73c893a7149c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_model = \"/content/model.pth\"\n",
        "path_to_output = \"/content/my\" #crea la cartella \"my\""
      ],
      "metadata": {
        "id": "wM1JJIxd1Zan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 2 (digits)"
      ],
      "metadata": {
        "id": "xzn0YrUaaWjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abbiamo 1797 campioni, ciascuno 8x8. Ogni immagine è organizzata come un vettore di 64 pixel\n",
        "digits_dataset = load_digits()\n",
        "\n",
        "print(digits_dataset.data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c2060c9-9f9e-4eff-e95b-ceb04e62147c",
        "id": "IWpMp1aOakYx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "#ogni riga è una immagine. Vediamo un esempio\n",
        "img1 = digits_dataset.data[0]\n",
        "#facciamo il reshape del vettore di 64 elementi in 8x8\n",
        "img1 = np.reshape(img1, (8,8))\n",
        "plt.imshow(img1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57897c51-5ea8-4bff-ce60-0597908713f7",
        "id": "zt0evHxEakYx"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f31aff2cb80>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#essendo ogni riga una immagine allora prendo le prime 1000 come training set, 350 come validation set e il resto come test set\n",
        "training_data = digits_dataset.data[0:1000].astype(np.float32)\n",
        "validation_data = digits_dataset.data[1000:1350].astype(np.float32)\n",
        "test_data = digits_dataset.data[1350:].astype(np.float32)"
      ],
      "metadata": {
        "id": "BQcWZ6-OakYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "input_shape_image = 8*8\n",
        "possible_pixel_values = 17 # ossia {0,1,...,16}"
      ],
      "metadata": {
        "id": "hGkkAp-2a3u5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 5 (Frey Face)"
      ],
      "metadata": {
        "id": "cuc9qvyWWHUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAR-zlTeWXRo",
        "outputId": "99ec9943-b432-4572-ae7f-e181b7282173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.io\n",
        "\n",
        "# Carica il file .mat\n",
        "data = scipy.io.loadmat('/content/drive/MyDrive/Generative_AI/datasets/frey_rawface.mat')"
      ],
      "metadata": {
        "id": "9FVLK-OdWJ81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data['ff'].T\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5fY0RULWy61",
        "outputId": "db5720ed-84e8-40f4-bb63-3c04c1db2b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 81, 136, 167, ..., 152, 158, 164],\n",
              "       [ 85, 138, 165, ..., 167, 178, 184],\n",
              "       [ 87, 139, 166, ..., 172, 177, 183],\n",
              "       ...,\n",
              "       [ 80,  84, 155, ..., 111, 186, 182],\n",
              "       [ 73,  86, 131, ..., 135, 167, 177],\n",
              "       [ 58,  89, 110, ..., 166, 177, 184]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 306
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.reshape(1965,28,20)"
      ],
      "metadata": {
        "id": "f-Yh3DqEXmDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "# Converte la matrice in un oggetto immagine di tipo Pillow\n",
        "image_list = [Image.fromarray(data[i]) for i in range(data.shape[0])]\n",
        "\n",
        "# Ridimensiona tutte le immagini nella lista all'altezza e larghezza desiderate\n",
        "new_image_list = [image.resize((28, 28)) for image in image_list]\n",
        "\n",
        "# Converti le immagini ridimensionate in una nuova matrice\n",
        "resized_data = np.array([np.array(image) for image in new_image_list])\n",
        "\n",
        "max_value = 20\n",
        "#normalizza\n",
        "img_normalized = np.round((resized_data / 255) * max_value).astype(int)"
      ],
      "metadata": {
        "id": "XVUkf3TTaOxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.rcParams['figure.figsize'] = (5, 5) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "#ogni riga è una immagine. Vediamo un esempio\n",
        "img1 = img_normalized[1120]\n",
        "print(img1.shape)\n",
        "plt.imshow(img1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "4352e611-3c75-4d67-b127-e37a46bbbfa1",
        "id": "wrA4figkX0fm"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(28, 28)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f40dde291e0>"
            ]
          },
          "metadata": {},
          "execution_count": 309
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#essendo ogni riga una immagine allora prendo le prime 1000 come training set, 350 come validation set e il resto come test set\n",
        "training_data = img_normalized[0:1300].astype(np.float32)\n",
        "validation_data = img_normalized[1300:1800].astype(np.float32)\n",
        "test_data = img_normalized[1800:].astype(np.float32)"
      ],
      "metadata": {
        "id": "JfhOkC2FX0fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "resize_to = 28\n",
        "input_shape_image = resize_to*resize_to\n",
        "possible_pixel_values = max_value+1 # ossia {0,1,...,max_value}"
      ],
      "metadata": {
        "id": "9csDpf8KX0fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset 6 (Celeb face)"
      ],
      "metadata": {
        "id": "-Q97vaC3NlqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5H0sxy_4QYm",
        "outputId": "e7d07043-6739-4c29-8c14-8e859f744e58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_model = \"/content/drive/MyDrive/Generative_AI/datasets/celebA/model/model_prior_mog_posterior_householder.pth\"\n",
        "path_to_output = \"/content/drive/MyDrive/Generative_AI/datasets/celebA/output/prior_mog_posterior_householder_\""
      ],
      "metadata": {
        "id": "rnxO1SFI4UQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocessing"
      ],
      "metadata": {
        "id": "QzT5im-Dog4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# Imposta il percorso della cartella contenente le immagini\n",
        "#scarica prima img_align_celeba.zip da https://drive.google.com/drive/u/0/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8?resourcekey=0-5BR16BdXnb8hVj6CNHKzLg\n",
        "folder_path = \"/content/drive/MyDrive/Generative_AI/datasets/celebA/all\"\n",
        "\n",
        "# Imposta le dimensioni di resize desiderate\n",
        "resize_to = 100  # Specifica la larghezza (W) e l'altezza (H)\n",
        "max_value = 20\n",
        "\n",
        "# Crea una lista per salvare le immagini pre-elaborate\n",
        "processed_images = []\n",
        "\n",
        "# Crea una trasformazione di pre-elaborazione utilizzando torchvision.transforms.Compose\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((resize_to,resize_to)),\n",
        "    transforms.Grayscale(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0, max_value),\n",
        "])\n",
        "\n",
        "# Itera sui file nella cartella\n",
        "i=0\n",
        "for file_name in os.listdir(folder_path):\n",
        "    print(i+1,\" -> \",file_name)\n",
        "    i=i+1\n",
        "    # Crea il percorso completo del file\n",
        "    file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "    # Carica l'immagine utilizzando PIL\n",
        "    image = Image.open(file_path)\n",
        "\n",
        "    # Applica le trasformazioni di pre-elaborazione all'immagine\n",
        "    processed_image = preprocess(image)\n",
        "\n",
        "    # Aggiungi l'immagine pre-elaborata alla lista\n",
        "    processed_images.append(processed_image)\n",
        "\n",
        "# Converte la lista di immagini in una matrice numpy\n",
        "image_matrix = np.stack(processed_images)\n",
        "\n",
        "# Ottieni le dimensioni della matrice delle immagini\n",
        "N, W, H = image_matrix.shape[0], image_matrix.shape[1], image_matrix.shape[2]\n",
        "\n",
        "# Visualizza le dimensioni della matrice delle immagini\n",
        "print(\"Dimensioni della matrice delle immagini:\", (N, W, H))\n"
      ],
      "metadata": {
        "id": "xRd1dvr1oh3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# Imposta il percorso del file di output\n",
        "file_path = \"/content/drive/MyDrive/Generative_AI/datasets/celebA/celebA.npy\"\n",
        "\n",
        "# Salva l'array nel file utilizzando np.save()\n",
        "np.save(file_path, processed_images)\n",
        "\n",
        "print(\"Array salvato correttamente.\")\n"
      ],
      "metadata": {
        "id": "jyQ99tm5c1WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/MyDrive/Generative_AI/datasets/celebA/celebA.npy\"\n",
        "dataset = np.load(file_path,allow_pickle=True)\n",
        "\n",
        "dataset.size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AE6ExjRUeg7x",
        "outputId": "9e587ad7-729a-49e0-f726-a44ab7273626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28638"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_matrix = np.stack(dataset)\n",
        "image_matrix.squeeze(1).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OpykpldjH3M",
        "outputId": "2af4f741-91a8-4ea1-cb96-214cad5eba2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28638, 100, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ottieni il numero totale di immagini\n",
        "num_images = image_matrix.shape[0]\n",
        "\n",
        "# Genera un array di indici per rimescolare le immagini\n",
        "shuffled_indices = np.arange(num_images)\n",
        "np.random.shuffle(shuffled_indices)\n",
        "\n",
        "# Rimescola le immagini\n",
        "image_matrix = image_matrix[shuffled_indices]"
      ],
      "metadata": {
        "id": "faXmVoMKQSYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import ndimage\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "resize_to = 50\n",
        "\n",
        "# Converti ndarray in tensor PyTorch\n",
        "input_tensor = torch.from_numpy(image_matrix)\n",
        "\n",
        "\n",
        "# Crea una trasformazione per il ridimensionamento\n",
        "resize = transforms.Resize((resize_to, resize_to))\n",
        "\n",
        "\n",
        "# Applica la trasformazione al tensor\n",
        "output_tensor = resize(input_tensor)\n",
        "\n",
        "# Converti il tensor in ndarray\n",
        "image_matrix = output_tensor.numpy()\n",
        "\n"
      ],
      "metadata": {
        "id": "32Jg1CjUlIvF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6ae921-a8e7-4bad-85b6-135380e4062e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_value = 20\n",
        "image_matrix = (image_matrix*20*max_value).astype(int)"
      ],
      "metadata": {
        "id": "sSE9KKbx-ObA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.rcParams['figure.figsize'] = (5, 5) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "#ogni riga è una immagine. Vediamo un esempio\n",
        "img1= image_matrix[5].squeeze(0)\n",
        "print(img1.shape)\n",
        "plt.imshow(img1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "5SPZNe28cDbm",
        "outputId": "b640034d-2fac-4852-d192-0aa106b18403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 100)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb68c09b790>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#essendo ogni riga una immagine allora prendo le prime 50000 come training set, 5000 come validation set e il resto come test set\n",
        "training_data = image_matrix.squeeze(1)[0:26000]\n",
        "validation_data = image_matrix.squeeze(1)[26000:]\n",
        "#test_data = d.data[3000:4000]"
      ],
      "metadata": {
        "id": "GZJWw1YHd4A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameters\n",
        "resize_to = 100 # se hai fatto il resize prima, commenta\n",
        "input_shape_image = resize_to*resize_to\n",
        "possible_pixel_values = max_value+1 # ossia {0,1,...,20}"
      ],
      "metadata": {
        "id": "hw4SGPg8d4BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "#elimino per liberare memoria\n",
        "del image_matrix\n",
        "del dataset\n",
        "#forzo il garbage collector a operare adesso\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-MNHWtvm2N_",
        "outputId": "99cab4f7-a21c-451a-a97f-38e20c42fc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader"
      ],
      "metadata": {
        "id": "TrqW_O67as26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non alleneremo la rete dandole tutti i dati, ma batch dopo batch. Creiamo quindi un DataLoader che semplicemente dividerà i dati in batch da 64 immagini (dopo averli mischiati) e ci restituirà, quando richiesto, un batch alla volta."
      ],
      "metadata": {
        "id": "mIp49CvLncr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=16, shuffle=True)\n",
        "#test_loader = DataLoader(test_data, batch_size=256, shuffle=True)"
      ],
      "metadata": {
        "id": "QO_kV43ulD3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model (adattato per gpu)"
      ],
      "metadata": {
        "id": "7iZsvZOfgOgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "hyperparameters"
      ],
      "metadata": {
        "id": "ddzRWuW7gSUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#definisco la dimensione dello spazio latente\n",
        "latent_space_dimension = 64 #deve essere pari se utilizzi il Flow-based\n",
        "#nuumero di hidden neurons nell'encoder e decoder\n",
        "number_of_hidden_neurons = 64 #la sua radice quadrata deve essere intera altrimenti ci sarà un errore\n",
        "#numero componenti gaussiane per il MoG o VampPrior\n",
        "mog_components = 20"
      ],
      "metadata": {
        "id": "AlmydjlygSUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mixture Of Gaussians (MoG)"
      ],
      "metadata": {
        "id": "izx9aBJoD75n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MoG(nn.Module):\n",
        "  def __init__(self, latent_dimension, num_components=1,):\n",
        "    super(MoG, self).__init__()\n",
        "\n",
        "    self.num_components = num_components\n",
        "    self.latent_dimension = latent_dimension\n",
        "\n",
        "    #inizializzo dei tensori \"da imparare\" che sono le medie delle K componenti\n",
        "    # di dimensione (Num_components, latent_dimension)\n",
        "    self.means = nn.Parameter(torch.randn(num_components, latent_dimension))\n",
        "\n",
        "    #inizializzo K matrici di covarianza diagonali di ognuna delle K componenti\n",
        "    #(Num_componenti, latent_dimension, latent_dimension)\n",
        "    #self.vars = nn.Parameter(torch.tensor(torch.eye(latent_dimension).unsqueeze(0).expand(num_components,-1,-1)))\n",
        "    #li tratto come log_var cosi che se anche fossero negativi, una volta fatto l'exp e quindi convertiti in var diventano positivi\n",
        "    self.log_vars = nn.Parameter(torch.randn((num_components,latent_dimension)))\n",
        "\n",
        "    #inizializzo i pesi di ogni gaussiana e li normalizzo affinchè la somma faccia 1\n",
        "    self.weights = nn.Parameter(torch.ones(num_components)/num_components )\n",
        "\n",
        "\n",
        "  def sample(self):\n",
        "\n",
        "    #campiono una camponente in base ai pesi\n",
        "    component_index =  torch.multinomial(F.softmax(self.weights, dim=0), 1)\n",
        "\n",
        "    #scelgo la media e la matrice di covarianza della componente scelta\n",
        "    mean = self.means[component_index]\n",
        "    log_var = self.log_vars[component_index]\n",
        "\n",
        "    #creo la matrice di covarianza a partire dai vettori che ne definiscono le diagonali\n",
        "    cov_matrix = torch.diag_embed(torch.exp(log_var))\n",
        "\n",
        "    #creo la multivariance\n",
        "    m = MultivariateNormal(mean,cov_matrix)\n",
        "\n",
        "    #campiono lo z\n",
        "    z_sample = m.sample().to(device)\n",
        "\n",
        "    return z_sample\n",
        "\n",
        "\n",
        "\n",
        "  def log_prob(self,z_samples):\n",
        "\n",
        "    #creo le matrici di covarianza a partire dai vettori che ne definiscono le diagonali\n",
        "    cov_matrices = torch.diag_embed(torch.exp(self.log_vars))\n",
        "\n",
        "    #creo K gaussiane mixate\n",
        "    MoG = MultivariateNormal(self.means.to(device),cov_matrices)\n",
        "\n",
        "    #reshape da (L, N, latent) in (N, latent)\n",
        "    z_reshaped = z_samples.view(-1, self.latent_dimension)\n",
        "\n",
        "    #Calcolo per ogni z le k log_prob (N, k)\n",
        "    k_log_probs_for_z = MoG.log_prob(z_reshaped.unsqueeze(1))\n",
        "\n",
        "    #Reshape originale (L, N, k)\n",
        "    k_log_probs_for_z_reshaped = k_log_probs_for_z.view(z_samples.shape[0],z_samples.shape[1], self.num_components)\n",
        "\n",
        "    #normalizzo i pesi affinchè la loro somma faccia 1\n",
        "    probabilities_weights = F.softmax(self.weights, dim=0)\n",
        "\n",
        "    #per ciascuno moltiplico le k probabilità per i rispettivi pesi\n",
        "    weigthed_log_probs = k_log_probs_for_z_reshaped * probabilities_weights\n",
        "\n",
        "    #sommo tutte le log_probs pesate di ogni z (L, N)\n",
        "    sum_weigthed_log_probs = weigthed_log_probs.sum(-1)\n",
        "\n",
        "    return sum_weigthed_log_probs"
      ],
      "metadata": {
        "id": "4G2tNO90D75n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VampPrior: Variational Mixture of Posterior Prior"
      ],
      "metadata": {
        "id": "J87_A_TwZ0wT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VampPrior(nn.Module):\n",
        "  def __init__(self, input_shape, latent_dimension, possible_pixel_values,encode= None, num_components=1):\n",
        "    super(VampPrior, self).__init__()\n",
        "\n",
        "    #sarebbe la dimensione D*D dell'ingresso originale a cui le immagini appartengono\n",
        "    self.input_shape = input_shape\n",
        "    self.num_components = num_components\n",
        "    self.latent_dimension = latent_dimension\n",
        "    self.encode = encode\n",
        "\n",
        "    #inizializzo gli pseudo-input\n",
        "    #creo N pseudo input u (N, sqrt(input_shape), sqrt(input_shape))\n",
        "    u = torch.rand((num_components, int(math.sqrt(input_shape)), int(math.sqrt(input_shape))))*possible_pixel_values\n",
        "    #li rendo learnable\n",
        "    self.u = nn.Parameter(u)\n",
        "\n",
        "    #inizializzo i pesi di ogni gaussiana e li normalizzo affinchè la somma faccia 1\n",
        "    self.weights = nn.Parameter(torch.ones(num_components)/num_components )\n",
        "\n",
        "\n",
        "  def sample(self):\n",
        "\n",
        "    #campiono una camponente in base ai pesi\n",
        "    component_index =  torch.multinomial(F.softmax(self.weights, dim=0), 1)\n",
        "\n",
        "    #do all'encoder gli pseudo-input ottenendo le medie e le log_std\n",
        "    mean_vectors, log_std_vectors = self.encode(self.u)\n",
        "\n",
        "    #scelgo la media e la matrice di covarianza della componente scelta\n",
        "    mean_vector = mean_vectors[component_index]\n",
        "    log_std_vector = log_std_vectors[component_index]\n",
        "\n",
        "    #creo la matrice di covarianza a partire dai vettori che ne definiscono le diagonali\n",
        "    cov_matrix = torch.diag_embed(torch.exp(log_std_vector))\n",
        "\n",
        "    #creo la multivariance\n",
        "    m = MultivariateNormal(mean_vector,cov_matrix)\n",
        "\n",
        "    #campiono lo z\n",
        "    z_sample = m.sample().to(device)\n",
        "\n",
        "    return z_sample\n",
        "\n",
        "\n",
        "\n",
        "  def log_prob(self,z_samples):\n",
        "\n",
        "    #do all'encoder gli pseudo-input ottenendo le medie e le log_std\n",
        "    #reshape degli pseudo-input da (N, sqrt(input_shape), sqrt(input_shape)) a (N, input_shape)\n",
        "    #x = torch.flatten(self.u,1)\n",
        "    #output = self.encoder.encoder(x)\n",
        "    #divido il risultato in due parti: media e std (diagonale)(logaritmica)\n",
        "    #mean_vectors, log_std_vectors = torch.chunk(output, 2, dim=1)\n",
        "\n",
        "    mean_vectors, log_std_vectors = self.encode(self.u)\n",
        "\n",
        "    #creo le matrici di covarianza a partire dai vettori che ne definiscono le diagonali\n",
        "    cov_matrices = torch.diag_embed(torch.exp(log_std_vectors))\n",
        "\n",
        "    #creo K gaussiane mixate\n",
        "    MoG = MultivariateNormal(mean_vectors,cov_matrices)\n",
        "\n",
        "    #reshape da (L, N, latent) in (N, latent)\n",
        "    z_reshaped = z_samples.view(-1, self.latent_dimension)\n",
        "\n",
        "    #Calcolo per ogni z le k log_prob (N, k)\n",
        "    k_log_probs_for_z = MoG.log_prob(z_reshaped.unsqueeze(1))\n",
        "\n",
        "    #Reshape originale (L, N, k)\n",
        "    k_log_probs_for_z_reshaped = k_log_probs_for_z.view(z_samples.shape[0],z_samples.shape[1], self.num_components)\n",
        "\n",
        "    #normalizzo i pesi affinchè la loro somma faccia 1\n",
        "    probabilities_weights = F.softmax(self.weights, dim=0)\n",
        "\n",
        "    #per ciascuno moltiplico le k probabilità per i rispettivi pesi\n",
        "    weigthed_log_probs = k_log_probs_for_z_reshaped * probabilities_weights\n",
        "\n",
        "    #sommo tutte le log_probs pesate di ogni z (L, N)\n",
        "    sum_weigthed_log_probs = weigthed_log_probs.sum(-1)\n",
        "\n",
        "    return sum_weigthed_log_probs\n"
      ],
      "metadata": {
        "id": "1NjHF-U3bNAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GTM-VampPrior:  Generative Topographic Mapping and Variational Mixture of Posterior Prior"
      ],
      "metadata": {
        "id": "b64mR5XyZRHL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GTM_VampPrior(nn.Module):\n",
        "  def __init__(self, input_shape, latent_dimension, possible_pixel_values,encode= None, num_components=1, u_dim=10):\n",
        "    super(GTM_VampPrior, self).__init__()\n",
        "\n",
        "    #sarebbe la dimensione D*D dell'ingresso originale a cui le immagini appartengono\n",
        "    self.input_shape = input_shape\n",
        "    self.num_components = num_components\n",
        "    self.latent_dimension = latent_dimension\n",
        "    self.encode = encode\n",
        "\n",
        "    #creo la rete che implementerà la funzione g che opera sui pseudo inputs\n",
        "    self.g_net = nn.Sequential(nn.Linear(u_dim*u_dim,number_of_hidden_neurons*2),\n",
        "                                 nn.BatchNorm1d(number_of_hidden_neurons*2),\n",
        "                                 nn.LeakyReLU(),\n",
        "                                 nn.Linear(number_of_hidden_neurons*2,number_of_hidden_neurons),\n",
        "                                 nn.BatchNorm1d(number_of_hidden_neurons),\n",
        "                                 nn.Tanh(),\n",
        "                                 #moltiplico per 2 perchè voglio sia il vettore di media che std (diagonale)\n",
        "                                 nn.Linear(number_of_hidden_neurons,input_shape),\n",
        "                                 nn.Sigmoid()\n",
        "                                 )\n",
        "\n",
        "    #inizializzo gli pseudo-input\n",
        "    #creo N pseudo input u (N, 10,10)\n",
        "    u = torch.rand((num_components, u_dim,u_dim))\n",
        "    #li rendo learnable\n",
        "    self.u = nn.Parameter(u)\n",
        "\n",
        "    #inizializzo i pesi di ogni gaussiana e li normalizzo affinchè la somma faccia 1\n",
        "    self.weights = nn.Parameter(torch.ones(num_components)/num_components )\n",
        "\n",
        "\n",
        "  def sample(self):\n",
        "\n",
        "    #campiono una camponente in base ai pesi\n",
        "    component_index =  torch.multinomial(F.softmax(self.weights, dim=0), 1)\n",
        "\n",
        "    #do all'encoder gli pseudo-input ottenendo le medie e le log_std\n",
        "    #processo gli pseudo-input con una funzion g\n",
        "    x = torch.flatten(self.u,1)\n",
        "    pseudo_input_after_g = self.g_net(x)\n",
        "\n",
        "    mean_vectors, log_std_vectors = self.encode(pseudo_input_after_g*possible_pixel_values)\n",
        "\n",
        "\n",
        "    #scelgo la media e la matrice di covarianza della componente scelta\n",
        "    mean_vector = mean_vectors[component_index]\n",
        "    log_std_vector = log_std_vectors[component_index]\n",
        "\n",
        "    #creo la matrice di covarianza a partire dai vettori che ne definiscono le diagonali\n",
        "    cov_matrix = torch.diag_embed(torch.exp(log_std_vector))\n",
        "\n",
        "    #creo la multivariance\n",
        "    m = MultivariateNormal(mean_vector,cov_matrix)\n",
        "\n",
        "    #campiono lo z\n",
        "    z_sample = m.sample().to(device)\n",
        "\n",
        "    return z_sample\n",
        "\n",
        "\n",
        "\n",
        "  def log_prob(self,z_samples):\n",
        "\n",
        "    #do all'encoder gli pseudo-input ottenendo le medie e le log_std\n",
        "    #processo gli pseudo-input con una funzion g\n",
        "    x = torch.flatten(self.u,1)\n",
        "    pseudo_input_after_g = self.g_net(x)\n",
        "\n",
        "    mean_vectors, log_std_vectors = self.encode(pseudo_input_after_g*possible_pixel_values)\n",
        "\n",
        "    #creo le matrici di covarianza a partire dai vettori che ne definiscono le diagonali\n",
        "    cov_matrices = torch.diag_embed(torch.exp(log_std_vectors))\n",
        "\n",
        "    #creo K gaussiane mixate\n",
        "    MoG = MultivariateNormal(mean_vectors,cov_matrices)\n",
        "\n",
        "    #reshape da (L, N, latent) in (N, latent)\n",
        "    z_reshaped = z_samples.view(-1, self.latent_dimension)\n",
        "\n",
        "    #Calcolo per ogni z le k log_prob (N, k)\n",
        "    k_log_probs_for_z = MoG.log_prob(z_reshaped.unsqueeze(1))\n",
        "\n",
        "    #Reshape originale (L, N, k)\n",
        "    k_log_probs_for_z_reshaped = k_log_probs_for_z.view(z_samples.shape[0],z_samples.shape[1], self.num_components)\n",
        "\n",
        "    #normalizzo i pesi affinchè la loro somma faccia 1\n",
        "    probabilities_weights = F.softmax(self.weights, dim=0)\n",
        "\n",
        "    #per ciascuno moltiplico le k probabilità per i rispettivi pesi\n",
        "    weigthed_log_probs = k_log_probs_for_z_reshaped * probabilities_weights\n",
        "\n",
        "    #sommo tutte le log_probs pesate di ogni z (L, N)\n",
        "    sum_weigthed_log_probs = weigthed_log_probs.sum(-1)\n",
        "\n",
        "    return sum_weigthed_log_probs\n"
      ],
      "metadata": {
        "id": "b07IG-ZPZRHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flow-based prior"
      ],
      "metadata": {
        "id": "4v7hTyqtgBO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Flow_Based_prior(nn.Module):\n",
        "  def __init__(self, latent_dimension):\n",
        "    super(Flow_Based_prior, self).__init__()\n",
        "\n",
        "    #divideremo l'input Z a metà, quindi prenderemo metà delle componenti\n",
        "    self.input_dimension = latent_dimension //2\n",
        "\n",
        "    self.number_of_neurons = 128\n",
        "    self.number_of_flows = 8\n",
        "\n",
        "    self.scale_net = nn.Sequential(nn.Linear(self.input_dimension,self.number_of_neurons),\n",
        "                                   nn.ELU(),\n",
        "                                   nn.Linear(self.number_of_neurons,self.number_of_neurons*2),\n",
        "                                   nn.ELU(),\n",
        "                                   nn.Linear(self.number_of_neurons*2,self.number_of_neurons*2),\n",
        "                                   nn.Tanh(),\n",
        "                                   nn.Linear(self.number_of_neurons*2,self.input_dimension),\n",
        "                                   nn.Tanh()\n",
        "                                   )\n",
        "    #neo creo 8\n",
        "    self.scale_nets = torch.nn.ModuleList([self.scale_net for _ in range(self.number_of_flows)])\n",
        "\n",
        "    self.translation_net = nn.Sequential(nn.Linear(self.input_dimension,self.number_of_neurons),\n",
        "                                nn.LeakyReLU(),\n",
        "                                nn.Linear(self.number_of_neurons,self.number_of_neurons*2),\n",
        "                                nn.ELU(),\n",
        "                                nn.Linear(self.number_of_neurons*2,self.number_of_neurons*2),\n",
        "                                nn.Tanh(),\n",
        "                                nn.Linear(self.number_of_neurons*2,self.input_dimension),\n",
        "                                )\n",
        "\n",
        "    #neo creo 8\n",
        "    self.translation_nets = torch.nn.ModuleList([self.translation_net for _ in range(self.number_of_flows)])\n",
        "\n",
        "    #la distribuzione iniziale da cui partire, ossia N(0,I)\n",
        "    self.p0 = MultivariateNormal(torch.zeros(latent_dimension).to(device), torch.eye(latent_dimension).to(device))\n",
        "\n",
        "  def coupling_layer(self, z, index, forward=True):\n",
        "\n",
        "    #divido l'input in due parti\n",
        "    (za,zb) = torch.chunk(z,2,1)\n",
        "    #print(\" Divido input: za=\", za, \" shape=\", za.shape)\n",
        "    #print(\" zb=\",zb)\n",
        "\n",
        "    #inizializzo i due output del coupling layer\n",
        "    ya = 0,\n",
        "    yb = 0\n",
        "\n",
        "    #print(\" calcolo s e t con ingresso di dimensione \", za.shape)\n",
        "    s = self.scale_nets[index](za)\n",
        "    t = self.translation_nets[index](za)\n",
        "\n",
        "    #print(\" s=\",s)\n",
        "    #print(\" t=\",t)\n",
        "\n",
        "    ya= za\n",
        "\n",
        "    if forward == False:\n",
        "      yb = torch.exp(s)*zb + t\n",
        "    else:\n",
        "      yb = (zb-t)*torch.exp(-s)\n",
        "\n",
        "    return torch.cat((ya,yb), 1), s\n",
        "\n",
        "  def permute(self, z):\n",
        "    return z.flip(1)\n",
        "\n",
        "  def log_prob(self,z):\n",
        "    '''\n",
        "      Io voglio calcolare il log(p(z)) e so che questo è calcolabile come:\n",
        "        log(p(z)) = ln(p0(z0=f^-1(x)) ) - sum(ln(det(J_fi(z_i-1))))\n",
        "    '''\n",
        "    #in ingresso ho (L, N, latent), lo converto in (L*N, latent)\n",
        "    L = z.shape[0]\n",
        "    N = z.shape[1]\n",
        "    z = z.view((L*N,z.shape[2]))\n",
        "    #se ho N z allora ho N log_det_J da memorizzare, uno per ogni z\n",
        "    log_det_J = z.new_zeros(z.shape[0])\n",
        "    #print(\"log_det_J: \",log_det_J)\n",
        "    output = z\n",
        "    #vado da p(x) a p0\n",
        "    for flow_i in range(self.number_of_flows):\n",
        "      #print(\"flow_i: \", flow_i)\n",
        "      output, s = self.coupling_layer(output, flow_i, forward=True)\n",
        "      output = self.permute(output)\n",
        "      log_det_J = log_det_J + s.sum(dim=1)\n",
        "\n",
        "    #adesso ho ottenuto che output=z0=f^-1(x) e ho la somma dei logaritmi dei determinanti\n",
        "    ln_p_z = self.p0.log_prob(output) - log_det_J\n",
        "    #print(\"Ln_p_z=\", ln_p_z)\n",
        "\n",
        "    #ritorno nel formato previsto (L, N)\n",
        "    return ln_p_z.view((L,N))\n",
        "\n",
        "\n",
        "  def sample(self):\n",
        "\n",
        "    #campiono dalla prior\n",
        "    z0 = self.p0.sample()\n",
        "\n",
        "    z = z0.unsqueeze(0)\n",
        "\n",
        "    #procedo da p0 a p(x)\n",
        "    for flow_i in reversed(range(self.number_of_flows)):\n",
        "      z = self.permute(z)\n",
        "      z,_ = self.coupling_layer(z, flow_i, forward=False)\n",
        "\n",
        "    #ritorno un campione di p(x)\n",
        "    return z\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CxdR5BEagCh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder"
      ],
      "metadata": {
        "id": "AHj2x6M4gSUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.linalg import multi_dot\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_shape_image, latent_space_dimension, number_of_hidden_neurons, L, T=10):\n",
        "    super(Encoder,self).__init__()\n",
        "\n",
        "    #numero di campioni per l'approssimazione Monte-Carlo dell'Expected Value\n",
        "    self.L = L\n",
        "\n",
        "    #indica il numero di HouseHolder matrices da utilizzare\n",
        "    self.T = T\n",
        "\n",
        "    self.input_shape_image = input_shape_image\n",
        "\n",
        "    self.latent_space_dimension = latent_space_dimension\n",
        "\n",
        "    out_channels = 8\n",
        "    kernel_size = 3\n",
        "\n",
        "    self.encoder = nn.Sequential(nn.Conv2d(1,3*out_channels,kernel_size,padding=\"same\"),\n",
        "                   nn.BatchNorm2d(3*out_channels),\n",
        "                   nn.LeakyReLU(),\n",
        "                   nn.Dropout(0.25),\n",
        "\n",
        "                   nn.Conv2d(3*out_channels,2*out_channels,kernel_size,padding=\"same\"),\n",
        "                   nn.BatchNorm2d(2*out_channels),\n",
        "                   nn.LeakyReLU(),\n",
        "                   nn.Dropout(0.25),\n",
        "\n",
        "                   nn.Conv2d(2*out_channels,out_channels,kernel_size,padding=\"same\"),\n",
        "                   nn.BatchNorm2d(out_channels),\n",
        "                   nn.LeakyReLU(),\n",
        "                   nn.Dropout(0.25),\n",
        "\n",
        "                   nn.Flatten(),\n",
        "                   nn.Linear(input_shape_image*out_channels,2*latent_space_dimension))\n",
        "\n",
        "    self.prior = None\n",
        "\n",
        "    #creo T vettori di Householder da imparare\n",
        "    v = torch.rand(T,latent_space_dimension, dtype=torch.float32)\n",
        "    self.householder_vectors = nn.Parameter(v)\n",
        "\n",
        "\n",
        "\n",
        "  def set_prior(self,prior):\n",
        "    #associo la prior scelta\n",
        "    self.prior = prior\n",
        "\n",
        "  def encode(self,x):\n",
        "    #trasformo il batch da (64, 28, 28) in (64,1,28,28)\n",
        "    x = x.unsqueeze(1)\n",
        "    #do alla rete x e prelevo vettore di media e std (diagonale)\n",
        "    output = self.encoder(x)\n",
        "\n",
        "    #divido il risultato in due parti: media e std (diagonale)(logaritmica)\n",
        "    mean_vector, log_std_vector = torch.chunk(output, 2, dim=1)\n",
        "\n",
        "    return mean_vector, log_std_vector\n",
        "\n",
        "\n",
        "  def KL_loss(self,log_std_vector,mean_vector, batch_length):\n",
        "\n",
        "    L = self.L\n",
        "\n",
        "    #print(\"Mean-vector\", str(mean_vector), \"  shape:\",mean_vector.shape)\n",
        "    #print(\"log-Std-vector\", str(log_std_vector), \"  shape:\",log_std_vector.shape)\n",
        "\n",
        "    #trasformo i logaritmi delle std in std\n",
        "    std_vector = torch.exp(log_std_vector)\n",
        "\n",
        "    #print(\"Std vectors: \",std_vector.device)\n",
        "\n",
        "    #siccome devo avere una matrice positiva definita (cholesky decomposition) devo\n",
        "    #assicurarmi che i valori nelle diagonali non siano proprio zero\n",
        "    EPS = 1.e-5\n",
        "    std_vector = torch.clamp(std_vector, EPS,1. - EPS)\n",
        "\n",
        "    #trasformo le sequenze di varianze in matrici diagonali (covarianza)\n",
        "    covariance_matrixes = torch.diag_embed(std_vector)\n",
        "\n",
        "    #print(\"Matrici di covarianza\",covariance_matrixes.device, \"  di shape \", covariance_matrixes.shape )\n",
        "\n",
        "    #calcolo N distribuzioni  multivariate q(z|x) creata da ognuno degli N x\n",
        "    q_z_x = MultivariateNormal(mean_vector, covariance_matrixes)\n",
        "\n",
        "    '''\n",
        "      per ciascuna distribuzione campiono L vettori z\n",
        "      Nota però che anche se campiono L vettori da ciascuna, il risultato\n",
        "      conterrà i primi N vettori z campionati, poi i secondi N e cosi via fino\n",
        "      agli L-esimi. Per esempio i primi due z1 e z2 sono stati campionati da due\n",
        "      distribuzioni diverse! Quindi non ho blocchi da L vettori z appartenenti\n",
        "      alla stessa distribuzione!\n",
        "    '''\n",
        "    #dimensione (L, num_distribuzioni, dim_latente)\n",
        "    z_samples = q_z_x.rsample((L,)) #r sta per \"reparametrization trick\"\n",
        "    #print(\"samples: \", z_samples.device)\n",
        "    #print(\"Shape samples: \", z_samples.shape)\n",
        "\n",
        "    '''\n",
        "      calcolo per ciascuno e sulla rispettiva distribuzione il log della prob\n",
        "      Ho una matrice (L, N), dove ogni \"colonna\" contiene le probabilità degli\n",
        "      z campionati dalla \"stessa\" distribuzione.\n",
        "    '''\n",
        "    z_log_probs = q_z_x.log_prob(z_samples)\n",
        "    #print(\"log q(z|x)\",z_log_probs)\n",
        "    #print(\"Shape q(z|x): \", z_log_probs.shape)\n",
        "\n",
        "    '''\n",
        "      Per ogni sample z calcolo la p(z). Otterrò una matrice (L, N), dove ogni\n",
        "      \"colonna\" contiene le probabilità degli z campionati dalla \"stessa\" distribuzione q(z|x)\n",
        "    '''\n",
        "    #creo la prior p(z)=N(z|0,I), è sempre la stessa\n",
        "    #p_z = MultivariateNormal(torch.zeros(self.latent_space_dimension).to(device), torch.eye(self.latent_space_dimension).to(device))\n",
        "\n",
        "    #ln_p_z = p_z.log_prob(z_samples)\n",
        "    ln_p_z = self.prior.log_prob(z_samples)\n",
        "\n",
        "    #print(\"ln_p(z)=\",ln_p_z.device)\n",
        "    #print(\"Shape di ln(p(z)) \",ln_p_z.shape)\n",
        "\n",
        "    '''\n",
        "      Ora per ogni immagine x io ho campionato L vettori z e per ciascuno ho\n",
        "      valutato sia ln(q(z|x)) che ln(p(z)). Per ogni immagine io volevo calcolare\n",
        "      l'expected value approssimandolo (Monte Carlo) come:\n",
        "\n",
        "                        KL = [Sum(ln(q(z|x)))/L - Sum(ln(p(z))/L)\n",
        "\n",
        "      Per ottenere la prima sommatoria, sommo le colonne di z_log_probs, mentre\n",
        "      per la seconda sommo le colonne della matrice ln_p_z. Dopodichè, ottenuti\n",
        "      due vettori, li divido per L e li sottraggo tra cosi da ottenere l'approssimazione\n",
        "      della KL per ogni immagine x in ingresso\n",
        "    '''\n",
        "\n",
        "    KL_per_image = z_log_probs.sum(0)/L - ln_p_z.sum(0)/L\n",
        "    #print(\"KL_per_image\",KL_per_image.device)\n",
        "\n",
        "    return KL_per_image,z_samples\n",
        "\n",
        "  '''\n",
        "    L'encoder ha rilasciato gli householder_vectors. Se sono T allora calcolerò\n",
        "    T matrici di Householder, le moltiplicherò tra loro per ottenere U. Dopodichè\n",
        "    trasformerò gli z campionati da p0 in degli z' come se z' fosse campionato da:\n",
        "                                  p(z')=N(u U, U diag(var^2) U^T)\n",
        "    quindi una distribuzione la cui matrice di covarianza non è limitata ad essere\n",
        "    una semplice diagonale permettendo quindi di rigettare l'ipotesi che le variabili\n",
        "    latenti siano indipendenti tra loro.\n",
        "  '''\n",
        "  def householder_transformation(self, z):\n",
        "\n",
        "    '''\n",
        "      Ricordo che dato un vi (Householder vector) la relativa matrice di Householder\n",
        "      Hi si trova come:\n",
        "                          H_i = I - 2* vi^T v / v v^T\n",
        "      Ne troveremo K che poi moltiplicheremo per trovare la matrice di trasformazione:\n",
        "                              U = H1*H2*...*HK\n",
        "      Dopodichè trasformeremo z cosi:\n",
        "                                  z' = U*z\n",
        "    '''\n",
        "\n",
        "    #creo la matrice identità\n",
        "    I = torch.eye(self.latent_space_dimension, dtype=torch.float32).to(device)\n",
        "\n",
        "    #per ogni householder vector vi calcolo calcolo ||vi||^2\n",
        "    norms = torch.norm(self.householder_vectors, dim=-1, keepdim=True)\n",
        "    dot_product = norms*norms\n",
        "\n",
        "    #print(\"House\",self.householder_vectors.shape)\n",
        "    #per ogni householder vector vi calcolo l'outer product con se stesso\n",
        "    outer_product = torch.matmul(self.householder_vectors.unsqueeze(2), self.householder_vectors.unsqueeze(1))\n",
        "\n",
        "    #adesso posso calcolare vi^T vi / ||vi||^2\n",
        "    normalized_outer_product = outer_product / dot_product[:,None]\n",
        "\n",
        "    #per ogni householder vector vi calcolo la relativa matrice di Householder Hi\n",
        "    H_is = I-2*normalized_outer_product\n",
        "\n",
        "    #calcolo latrasformazione U data dalla moltiplicazione H1*H2*...HT\n",
        "    U = H_is[0]\n",
        "    for i in np.arange(self.T-1):\n",
        "        U = torch.matmul(U,H_is[i+1])\n",
        "\n",
        "    #adesso moltiplico U per ogni campione z di p0\n",
        "    z_transformed = torch.matmul(U.unsqueeze(0),torch.transpose(z.unsqueeze(2),2,3)).squeeze(-1)\n",
        "\n",
        "\n",
        "    #ritorno (L, N, dim_latente) di vettori però trasformati\n",
        "    return z_transformed\n",
        "\n",
        "  def sample(self):\n",
        "    z_sample = self.prior.sample()\n",
        "    #prima di darlo al decoder, dato che è stato allenato con i campioni di pk\n",
        "    #dovrò trasformare z0 in zk\n",
        "    #siccome il codice è lo stesso, ma si aspetta (L, N, lantent_dim) allora faccio il reshape\n",
        "    z_sample = z_sample.unsqueeze(0)\n",
        "\n",
        "    transformed_z_sample = self.householder_transformation(z_sample)\n",
        "\n",
        "    return transformed_z_sample.squeeze(0)\n",
        "\n",
        "    #La rete ritorna il vettore di media, std (diagonale) e la z campionata\n",
        "  def forward(self, x):\n",
        "\n",
        "    #trasformo il batch da (64, 28, 28) in (64,1,28,28)\n",
        "    x = x.unsqueeze(1)\n",
        "    #print(\"X shape: \",x.shape)\n",
        "    #do alla rete x e prelevo vettore di media e std (diagonale)\n",
        "    output = self.encoder(x)\n",
        "\n",
        "    #print(output.device)\n",
        "    #print(\"Output shape encoder: \",output)\n",
        "    #divido il risultato in tre parti: media e std (diagonale)(logaritmica)\n",
        "    mean_vector, log_std_vector = torch.chunk(output, 2, dim=1)\n",
        "\n",
        "    #Il codice continua ad essere lo stesso dato che in termini di probabilità le cose non cambiano\n",
        "\n",
        "    '''\n",
        "      Ottengo un KL_error (N,) contenente per ogni immagine il relativo KL_error,\n",
        "      e poi una matrice z_samples (L, N, dim_latente)\n",
        "    '''\n",
        "    KL_per_image, z_samples = self.KL_loss(log_std_vector,mean_vector, x.shape[0])\n",
        "    #print(\"KL_per_image: \",KL_per_image)\n",
        "\n",
        "\n",
        "    '''\n",
        "        Questa è la parte nuova. Gli z_samples sono stati campionati da p0, adesso\n",
        "        voglio che il decoder decodifichi gli stessi z_samples però trasformati nello\n",
        "        spazio latente di Householder, ossia quello dove la matrice di covarianza è\n",
        "        full.\n",
        "    '''\n",
        "\n",
        "    transformed_z_samples = self.householder_transformation(z_samples)\n",
        "\n",
        "    return KL_per_image,transformed_z_samples"
      ],
      "metadata": {
        "id": "S1--Lvq2z4Bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "8XD2UZLYgSUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, input_shape_image,latent_space_dimension, number_of_hidden_neurons, possible_pixel_values, L):\n",
        "    super(Decoder,self).__init__()\n",
        "\n",
        "    self.L = L\n",
        "\n",
        "    self.input_shape_image = input_shape_image\n",
        "\n",
        "    self.latent_space_dimension = latent_space_dimension\n",
        "\n",
        "    self.possible_pixel_values = possible_pixel_values\n",
        "\n",
        "    kernel_size = 3\n",
        "    out_channels = 8\n",
        "    self.decoder = nn.Sequential(nn.Linear(latent_space_dimension,number_of_hidden_neurons),\n",
        "                                 nn.Unflatten(2, (int(math.sqrt(number_of_hidden_neurons)),int(math.sqrt(number_of_hidden_neurons)))),\n",
        "\n",
        "                                nn.ConvTranspose2d(1,out_channels,kernel_size),\n",
        "                                nn.LeakyReLU(),\n",
        "\n",
        "                                nn.ConvTranspose2d(out_channels,2*out_channels,kernel_size),\n",
        "                                nn.LeakyReLU(),\n",
        "\n",
        "                                nn.ConvTranspose2d(2*out_channels,3*out_channels,kernel_size),\n",
        "                                nn.LeakyReLU(),\n",
        "\n",
        "                                nn.Flatten(),\n",
        "                                nn.LazyLinear(input_shape_image*possible_pixel_values),\n",
        "                                nn.Unflatten(1,(input_shape_image,possible_pixel_values))\n",
        "                                )\n",
        "\n",
        "\n",
        "  def decode_sample(self,z_sample):\n",
        "    #creo la prior p(z)=N(z|0,I), è sempre la stessa\n",
        "    #p_z = MultivariateNormal(torch.zeros(self.latent_space_dimension), torch.eye(self.latent_space_dimension))\n",
        "    #z_sample = p_z.sample().to(device)\n",
        "    #print(\"z_sample \", z_sample.shape)\n",
        "\n",
        "\n",
        "    #inietto nel decoder:\n",
        "    z_sample = z_sample.reshape(1,1,z_sample.shape[1])\n",
        "\n",
        "    logits = self.decoder(z_sample)\n",
        "    #print(\"Shape logits in sample \", logits.shape)\n",
        "\n",
        "\n",
        "    #(1,784,256)\n",
        "    logits = logits.reshape(1, self.input_shape_image, self.possible_pixel_values )\n",
        "\n",
        "    probabilities = torch.softmax(logits, dim=-1)\n",
        "    #non applico la softmax per convertirli in probabilità perchè\n",
        "    #la multinomial di torch accetta anche le logits\n",
        "    probabilities = probabilities.view(-1, self.possible_pixel_values)\n",
        "\n",
        "    sample = torch.multinomial(probabilities, num_samples=1)\n",
        "\n",
        "    x = sample.view(self.input_shape_image)\n",
        "    return x\n",
        "\n",
        "\n",
        "  def forward(self, z, x):\n",
        "    #z è una matrice (L, N, dim_latente), la inietto nel decoder per ottenere\n",
        "    #le logits\n",
        "    #print(\"Decoder forward\")\n",
        "\n",
        "    #Le logits hanno forma (L,N, numero_pixel*possibili_valori)\n",
        "    z = z.reshape(z.shape[0]*z.shape[1],1,z.shape[2])\n",
        "    #print(\"Z dimension: \", z.shape)\n",
        "    logits = self.decoder(z).to(device)\n",
        "    #print(\"logits: \", logits.shape)\n",
        "    logits = logits.reshape((z.shape[0],z.shape[1],logits.shape[1],logits.shape[2]))\n",
        "    #print(\"logits resh: \", logits)\n",
        "    '''\n",
        "      Prima di convertire le logits in probabilità, ciascun vettore del tensore\n",
        "      contiene i logits di TUTTI i pixel [px1-v=v1,...,px1-v=vk, px2-v=1,....], quindi\n",
        "      devo prima fare un reshape del genere [[px1-v=v1,...,px1-v=vk], [...]] isolando\n",
        "      solo le probabilità di ogni pixel\n",
        "\n",
        "    '''\n",
        "    #(L, N, numero_pixel, possibili_valori)\n",
        "    logits = logits.reshape((logits.shape[0],logits.shape[1],self.input_shape_image,self.possible_pixel_values))\n",
        "    #applico la softmax per convertire le logits in probabilità\n",
        "    probabilities = torch.softmax(logits,3)\n",
        "    #print(\"probabilities\",probabilities.device)\n",
        "\n",
        "    #correggo (per questioni di stabilità) le probabilità troppo basse\n",
        "    #Devono stare tra 0+EPS < p < 1-EPS\n",
        "    EPS = 1.e-5\n",
        "    probabilities = torch.clamp(probabilities, EPS,1. - EPS)\n",
        "    #print(\"probabilities \",probabilities.device)\n",
        "    '''\n",
        "      Per ogni z iniettato ho ottenuto delle probabilità. Siccome voglio valutare\n",
        "      l'expected value seguente:\n",
        "                              E[ln(p(x|z))]\n",
        "      e sicome lo voglio approssimare con gli L ln(p(x|z)) ottenuti, ossia:\n",
        "                              E[ln(p(x|z))] = 1/L*Sum(ln(p(x|z)))\n",
        "      allora tutti i calcoli seguenti servono solo a poter ottenere per ciascuna\n",
        "      immagine x tutti i ln(p(x|z)), in particolare:\n",
        "      1) Per ogni z ho una matrice di dimensione(numero_pixel, probabilità_valori)\n",
        "         e quindi estraggo la probabilità che ha quella particolare componente xi\n",
        "         in ingresso.\n",
        "      2) Alla fine per ogni coppia x e z ho un vettore di probabilità per xi, quindi\n",
        "         quella di x è calcolabile come:\n",
        "                              p(x|z)=p(x1|z)*p(x2|z)*...*p(xk|z)\n",
        "         Se però calcolo il logaritmo, che è quello che voglio posso sommarli:\n",
        "                              ln(p(x|z))=ln(p(x1|z))+ln(p(x2|z))+...+ln(p(xk|z))\n",
        "      3) Avendone L li sommo e li divido per L\n",
        "\n",
        "    '''\n",
        "    #converto ogni pixel in un vettore one_hot\n",
        "    x_one_hot = F.one_hot(x.long(), num_classes = self.possible_pixel_values)\n",
        "    #print(\"ONE-HOT\", x_one_hot.device)\n",
        "    x_one_hot = x_one_hot.reshape(x_one_hot.shape[0],x_one_hot.shape[1]*x_one_hot.shape[2]*x_one_hot.shape[3])\n",
        "    #print(\"ONE-HOT reshaped \",x_one_hot.device)\n",
        "    probabilities = probabilities.reshape(probabilities.shape[0],probabilities.shape[1],probabilities.shape[2]*probabilities.shape[3])\n",
        "    #li converto in logaritmi\n",
        "    log_probabilities = torch.log(probabilities)\n",
        "    #print(\"Probabilities \", probabilities.device)\n",
        "    selected_log_probabilities = x_one_hot * log_probabilities\n",
        "    #adesso in un unico vettore ho tutti le ln(p(x_i|z)) per lo z.\n",
        "    #print(\"Log Selected probabilities \", selected_log_probabilities.device)\n",
        "    #li sommo (L,N), ossia ogni vettore contiene gli ln(p(x|z)) per gli N x\n",
        "    ln_p_x_z = selected_log_probabilities.sum(2)\n",
        "    #print(\"ln(p(x|z) \", ln_p_x_z.device)\n",
        "    #ogni colonna contiene quindi gli ln(p(x|z)) per lo stesso x.\n",
        "    #li sommo e li divido per L ottenenedo il reconstruction error per ogni x\n",
        "    RE_per_image = ln_p_x_z.sum(0) / self.L\n",
        "    #print(\"RE_per_image\",RE_per_image.device)\n",
        "\n",
        "    return RE_per_image\n",
        "\n"
      ],
      "metadata": {
        "id": "hRYe21JJgSUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VAE"
      ],
      "metadata": {
        "id": "M_VcPGCMgSUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, possible_pixel_values, input_shape_image, latent_space_dimension, number_of_hidden_neurons,L,T, type_of_prior=0, mog_components=1):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(input_shape_image,latent_space_dimension,number_of_hidden_neurons,L, T)\n",
        "    self.decoder = Decoder(input_shape_image,latent_space_dimension,number_of_hidden_neurons,possible_pixel_values,L)\n",
        "\n",
        "    prior = None\n",
        "    if type_of_prior == 1:\n",
        "      #creo una mixture of gaussian a 15 componenti come prior p(z)\n",
        "      prior = MoG(latent_space_dimension, mog_components)\n",
        "    elif type_of_prior == 2:\n",
        "      prior = VampPrior(input_shape_image, latent_space_dimension, possible_pixel_values, self.encoder.encode, num_components= mog_components )\n",
        "    elif type_of_prior == 3:\n",
        "      prior = GTM_VampPrior(input_shape_image, latent_space_dimension, possible_pixel_values, self.encoder.encode, num_components= mog_components )\n",
        "    elif type_of_prior == 4:\n",
        "      prior = Flow_Based_prior(latent_space_dimension)\n",
        "    else:\n",
        "      #p(z)=N(0,I)\n",
        "      prior = MultivariateNormal(torch.zeros(self.latent_space_dimension), torch.eye(self.latent_space_dimension))\n",
        "\n",
        "    self.prior = prior\n",
        "\n",
        "  def initialize(self):\n",
        "    #aggiorno il riferimento della prior per l'encoder\n",
        "    self.encoder.set_prior(self.prior)\n",
        "    #e dell'encoder per la prior\n",
        "    #self.prior.set_encoder(self.encoder)\n",
        "\n",
        "  def sample(self):\n",
        "    z_sample = self.encoder.sample()\n",
        "    return self.decoder.decode_sample(z_sample)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #inietto x nell'encoder per ottenere la KL loss e i vettori z campionati (Monte-Carlo)\n",
        "    KL_loss_per_image, z_samples_per_image = self.encoder.forward(x)\n",
        "\n",
        "    #inietto nel decoder x per essere ricostruito attraverso gli stessi campioni z\n",
        "    #e per ottenere il reconstruction error\n",
        "    RE_loss_per_image = self.decoder.forward(z_samples_per_image,x)\n",
        "\n",
        "    #sommo per ottenere una approssimazione del ln(p(x)) per ogni immagine\n",
        "    ln_p = KL_loss_per_image - RE_loss_per_image\n",
        "\n",
        "    #calcolo la media del batch\n",
        "    ln_p_mean = ln_p.mean()\n",
        "\n",
        "    return ln_p_mean"
      ],
      "metadata": {
        "id": "2v7SZ8r8gSUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "u_P94_AwVz8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L=2 #for Monte Carlo"
      ],
      "metadata": {
        "id": "SVL6n1H4gYJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = VAE( possible_pixel_values, input_shape_image, latent_space_dimension, number_of_hidden_neurons, L,T=12,type_of_prior=1, mog_components=mog_components).to(device)\n",
        "model.initialize()"
      ],
      "metadata": {
        "id": "sJc48paqV04o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d59280c-4fa5-48c9-b3a0-be82aa3234ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-3\n",
        "#i parametri che l'optimizer deve ottimizzare sono tutti quelli del modello\n",
        "parameters_to_optimize = [p for p in model.parameters() if p.requires_grad == True]\n",
        "\n",
        "optimizer = torch.optim.Adamax(parameters_to_optimize, lr=learning_rate)"
      ],
      "metadata": {
        "id": "58k7uDJ-WLID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def sample_and_save(model, name, input_shape):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  #voglio campionare 16 immagini e le voglio in una griglia 4x4\n",
        "  n=4\n",
        "  number_of_grid_cells = n*n\n",
        "  #quindi dico al modello di campionarmi 16 immagini\n",
        "  xs = np.zeros((number_of_grid_cells,input_shape))\n",
        "  for i in np.arange(number_of_grid_cells):\n",
        "    generated_sample = model.sample().cpu()\n",
        "    #lo stacco dal grafo di computazione\n",
        "    generated_sample = generated_sample.detach().numpy()\n",
        "    xs[i,:] = generated_sample\n",
        "\n",
        "\n",
        "  fig, ax = plt.subplots(n, n)\n",
        "  for i, ax in enumerate(ax.flatten()):\n",
        "      plottable_image = np.reshape(xs[i], (int(math.sqrt(input_shape)), int(math.sqrt(input_shape))))\n",
        "      ax.imshow(plottable_image, cmap='gray')\n",
        "      ax.axis('off')\n",
        "\n",
        "  plt.savefig(path_to_output+'epoca_' +str(name)+ '.pdf', bbox_inches='tight')\n",
        "  plt.close()\n"
      ],
      "metadata": {
        "id": "K0kD3JWPAdpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "number_of_epochs = 1000\n",
        "\n",
        "#qui salvo il migliore modello, ossia quello che ha la loss sulla validazione migliore\n",
        "best_model = model\n",
        "best_validation_loss = 1000000\n",
        "\n",
        "patience = 0\n",
        "max_patience = 25\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "  model.train()\n",
        "  print(\"Epoca \"+str(epoch)+\" _____________________________________________________________________\")\n",
        "  i=1\n",
        "  for index_batch, batch in enumerate(training_loader):\n",
        "\n",
        "    if len(batch.shape)==2:\n",
        "      batch = batch.reshape(batch.shape[0],int(math.sqrt(batch.shape[1])),int(math.sqrt(batch.shape[1])))\n",
        "    batch = batch.to(device)\n",
        "\n",
        "    batch = batch.to(torch.float32)\n",
        "\n",
        "    loss = model.forward(batch)\n",
        "\n",
        "    #calcolo le derivate parziali della loss rispetto ogni parametro\n",
        "    loss.backward()\n",
        "\n",
        "    #adesso ogni parametro ha in .grad il gradiente. Aggiorno il suo valore\n",
        "    optimizer.step()\n",
        "\n",
        "    #resetto il .grad di ogni parametro (altrimenti sommo quello attuale al successivo che calcoleremo nell'epoca dopo)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    print(\"   Loss batch: \",str(i),\": \", loss)\n",
        "    #print(\"----\",model.prior.u)\n",
        "    i=i+1\n",
        "\n",
        "  #alla fine di ogni epoca, valuto come si comporta la loss col validation set\n",
        "  print(\"   ___________________________\")\n",
        "  model.eval()\n",
        "  validation_loss = 0\n",
        "  N = 0\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  gc.collect()\n",
        "\n",
        "  for index_batch, batch in enumerate(validation_loader):\n",
        "    if len(batch.shape)==2:\n",
        "      batch = batch.reshape(batch.shape[0],int(math.sqrt(batch.shape[1])),int(math.sqrt(batch.shape[1])))\n",
        "    batch = batch.to(device)\n",
        "    batch = batch.to(torch.float32)\n",
        "    loss_i = model.forward(batch)\n",
        "    validation_loss = validation_loss + loss_i.item()#.to(\"cpu\")\n",
        "    N = N +  1#batch.shape[0]\n",
        "    print(\"   Loss validation batch \",str(N),\": \",loss_i)\n",
        "\n",
        "  validation_loss = validation_loss/N\n",
        "  print(\"   Loss media validation: \",str(validation_loss))\n",
        "\n",
        "  #se tale modello ha una loss migliore di quella attualmente migliore..\n",
        "  if validation_loss < best_validation_loss:\n",
        "    patience = 0\n",
        "    best_validation_loss = validation_loss\n",
        "    print(\"   la loss risulta essere migliore\")\n",
        "    torch.save(model.state_dict(), path_to_model)\n",
        "    #campiono e salvo\n",
        "    sample_and_save(model, epoch, input_shape_image)\n",
        "  else:\n",
        "    print(\"   patience= \"+ str(patience+1))\n",
        "    patience = patience + 1\n",
        "\n",
        "  if patience > max_patience:\n",
        "    print(\"\")\n",
        "    print(\"Patience massimo superato. Fine del training\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "wHHcmhvqWftP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}