{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "95Unj2pV1ELG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "YNDxmdd01P7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abbiamo 1797 campioni, ciascuno 8x8. Ogni immagine è organizzata come un vettore di 64 pixel\n",
        "digits_dataset = load_digits()\n",
        "\n",
        "print(digits_dataset.data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYPMHwO6k3_R",
        "outputId": "36cc6186-5907-465b-927b-54e2a889f787"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "#ogni riga è una immagine. Vediamo un esempio\n",
        "img1 = digits_dataset.data[0]\n",
        "#facciamo il reshape del vettore di 64 elementi in 8x8\n",
        "img1 = np.reshape(img1, (8,8))\n",
        "plt.imshow(img1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "xOzMXbwxl_7X",
        "outputId": "b6564c8f-9431-4316-d787-be2a00592f19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc38ee7ba90>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#essendo ogni riga una immagine allora prendo le prime 1000 come training set, 350 come validation set e il resto come test set\n",
        "training_data = digits_dataset.data[0:1000].astype(np.float32)\n",
        "validation_data = digits_dataset.data[1000:1350].astype(np.float32)\n",
        "test_data = digits_dataset.data[1350:].astype(np.float32)"
      ],
      "metadata": {
        "id": "rH56FdMIlpbN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non alleneremo la rete dandole tutti i dati, ma batch dopo batch. Creiamo quindi un DataLoader che semplicemente dividerà i dati in batch da 64 immagini (dopo averli mischiati) e ci restituirà, quando richiesto, un batch alla volta."
      ],
      "metadata": {
        "id": "mIp49CvLncr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "QO_kV43ulD3w"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "dHT3GO4xrS8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameters"
      ],
      "metadata": {
        "id": "edB8ADaWr2xM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#lunghezza di ogni immagine flattata. Essendo 8x8 --> 64\n",
        "length_flatted_image = 64\n",
        "\n",
        "#numero di hidden neuron nella rete scala s() e traslazione t()\n",
        "number_of_neurons_for_scale_and_translation_network = 256\n",
        "\n",
        "#numero di distribuzioni per approssimare la p(x) reale\n",
        "number_of_flows = 8\n",
        "\n"
      ],
      "metadata": {
        "id": "bUhxMhnIr4ZE"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scale and translation network"
      ],
      "metadata": {
        "id": "dxrUh1kut6SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Scale_Net(nn.Module):\n",
        "  def __init__(self, input_dimension, number_of_neurons_for_scale_and_translation_network):\n",
        "    super(Scale_Net, self).__init__()\n",
        "\n",
        "    #divideremo l'input in due parti uguali, quindi la scale net prenderà in ingresso\n",
        "    #la metà delle componenti\n",
        "    self.input_dimension = input_dimension // 2 \n",
        "    self.number_of_neurons = number_of_neurons_for_scale_and_translation_network\n",
        "\n",
        "    #indico i layer\n",
        "    self.linear_layer_1 = nn.Linear(self.input_dimension, self.number_of_neurons)\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "    self.linear_layer_2 = nn.Linear(self.number_of_neurons, self.number_of_neurons)\n",
        "    self.linear_layer_3 = nn.Linear(self.number_of_neurons, self.input_dimension)\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear_layer_1(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.linear_layer_2(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.linear_layer_3(x)\n",
        "    x = self.tanh(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class Translation_Net(nn.Module):\n",
        "  def __init__(self, input_dimension, number_of_neurons_for_scale_and_translation_network):\n",
        "    super(Translation_Net, self).__init__()\n",
        "\n",
        "    #divideremo l'input in due parti uguali, quindi la translation net prenderà in ingresso\n",
        "    #la metà delle componenti\n",
        "    self.input_dimension = input_dimension // 2 \n",
        "    self.number_of_neurons = number_of_neurons_for_scale_and_translation_network\n",
        "\n",
        "    #indico i layer\n",
        "    self.linear_layer_1 = nn.Linear(self.input_dimension, self.number_of_neurons)\n",
        "    self.leaky_relu = nn.LeakyReLU()\n",
        "    self.linear_layer_2 = nn.Linear(self.number_of_neurons, self.number_of_neurons)\n",
        "    self.linear_layer_3 = nn.Linear(self.number_of_neurons, self.input_dimension)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.linear_layer_1(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.linear_layer_2(x)\n",
        "    x = self.leaky_relu(x)\n",
        "    x = self.linear_layer_3(x)\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "xML5E_HNt9Sd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting distribution p0"
      ],
      "metadata": {
        "id": "HrnW-BdS3vtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p0 = torch.distributions.MultivariateNormal(torch.zeros(length_flatted_image), torch.eye(length_flatted_image))"
      ],
      "metadata": {
        "id": "NF-DoZc23zad"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RealNVP module"
      ],
      "metadata": {
        "id": "9YW-Mvti5SCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RealNVP(nn.Module):\n",
        "  def __init__(self, Scale_net, Translation_net, number_of_flows, p0, length_flatted_image, number_of_neurons_for_scale_and_translation_network):\n",
        "    super(RealNVP, self).__init__()\n",
        "    \n",
        "    self.p0 = p0\n",
        "\n",
        "    #creo un \"array\" di nn.Moduli contenente ciascuno le (es.) 8 scale e translation net\n",
        "    self.scale_networks = torch.nn.ModuleList([Scale_Net(length_flatted_image, number_of_neurons_for_scale_and_translation_network) for _ in range(number_of_flows)]) \n",
        "    self.translation_networks = torch.nn.ModuleList([Translation_Net(length_flatted_image, number_of_neurons_for_scale_and_translation_network) for _ in range(number_of_flows)])\n",
        "\n",
        "    self.number_of_flows = number_of_flows\n",
        "    self.length_flatted_image = length_flatted_image\n",
        "\n",
        "  '''\n",
        "  index serve a indicare in quale flow siamo\n",
        "  \n",
        "  forward indica, se True, che da px ci stiamo muovendo verso p0 (quindi dobbiamo usare le\n",
        "  funzioni inverse), altrimenti le funzioni dirette\n",
        "  '''\n",
        "  def coupling_layer(self, x, index, forward= True):\n",
        "    '''\n",
        "      NB: anche se chiamo xa e xb e poi ya e yb i nomi dipendono in realtà\n",
        "      dal tipo di forwording. Se è False allora rimangono cosi, altrimenti a \n",
        "      livello \"semantico\" andrebbero cambiati.\n",
        "    '''\n",
        "    print(x.shape)\n",
        "    #divido l'input in due parti\n",
        "    (xa,xb) = torch.chunk(x,2,1)\n",
        "\n",
        "    #inizializzo i due output del coupling layer\n",
        "    ya = 0\n",
        "    yb = 0\n",
        "\n",
        "    #indipendentemente dal forwarding calcolo il valore di s e t\n",
        "    #se siamo nel forwarding allora xa è da intendere come ya\n",
        "    s = self.scale_networks[index](xa)\n",
        "    t = self.translation_networks[index](xa)\n",
        "\n",
        "    #per trovare ya (indipendentemente dal forwarding)conosco già la f1,\n",
        "    # questa è la funzione identità su xa\n",
        "    ya = xa\n",
        "\n",
        "    '''\n",
        "      Se sto procedendo da p0 verso px (forward=False) allora so che:\n",
        "      yb = f2(xa, xb) = exp(s(xa)) Hadamard xb + t(xa)\n",
        "      \n",
        "    '''\n",
        "    #Non essendo nella fase di forward so che gli ingressi sono xa e xb\n",
        "    if forward == False:\n",
        "      yb = torch.exp(s)*xb + t   \n",
        "    else:\n",
        "      '''\n",
        "      Se sto procedendo da px verso p0 (forward=True) allora so che:\n",
        "      xb = f2^-1(ya, yb) = (yb-t(ya)) Hadamard exp(-s(ya))\n",
        "      Ma ripeto, qui xa e xb sono da interpretare come input\n",
        "      \n",
        "    '''\n",
        "      yb = (xb-t)*torch.exp(-s)\n",
        "\n",
        "    \n",
        "    #ritorniamo l'output come [y_a,y_b] e anche s perchè ci servirà per il\n",
        "    #calcolo della loss \n",
        "    return torch.cat((ya,yb),1), s\n",
        "\n",
        "  def permute(self,x):\n",
        "    #scambio componenti Es. da 1234 a 4321\n",
        "    return x.flip(1)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    '''\n",
        "    per il calcolo della loss dobbiamo memorizzarci per ogni immagine\n",
        "    il logaritmo della jacobiana. Lo aggiorneremo ad ogni flow\n",
        "    '''\n",
        "    log_det_J = x.new_zeros(x.shape[0])\n",
        "    output = x\n",
        "    # per ogni flow...(procedendo da px verso p0)\n",
        "    for flow_i in range(self.number_of_flows):\n",
        "      #iniettiamo x (se siamo all'inizio) o l'output precedente e \n",
        "      #otteniamo l'output (insieme a s) dal coupling layer\n",
        "      output, s = self.coupling_layer(output, flow_i, forward=True)\n",
        "      #effettuiamo la permutazione\n",
        "      output = self.permute(output)\n",
        "      #per ogni immagine aggiorniamo il logaritmo del determinante\n",
        "      log_det_J = log_det_J + s.sum(dim=1)\n",
        "\n",
        "    '''\n",
        "      Adesso in output ho l'output per ogni immagine iniettata + la somma\n",
        "      dei logaritmi dei determinanti della Jacobiana in ogni flow ottenuta.\n",
        "      Essendo ciascun output \"l'ultima\" trasformazione fatta, sarebbero le \n",
        "      componenti del vettore z0 da dare alla prior. Ottenuto il suo valore \n",
        "      abbiamo tutti gli ingredienti per calcolare la p(x)\n",
        "    '''\n",
        "    # La loss per ogni immagine è  -ln(p0(z0))+ sum(log_det_Ji)\n",
        "    # Avendone diverse, per il batch calcolo la media (o la somma)\n",
        "    mle_batch = (-self.p0.log_prob(output) + log_det_J).mean()\n",
        "\n",
        "    #ritorno la loss\n",
        "    return mle_batch\n",
        "\n",
        "\n",
        "  def sample_new_data(self):\n",
        "    #campiono dalla prior\n",
        "    z = self.p0.sample()\n",
        "    #aggiungo una dimensione per il batch cosi da poterla iniettare\n",
        "    #nella rete sfruttando stavolta le funzioni dirette! \n",
        "    z = z.unsqueeze(0)\n",
        "    #Inoltre devo percorrerla da sinistra verso destra, quindi cambio ordine nel for\n",
        "    for flow_i in reversed(range(self.number_of_flows)):\n",
        "      #stavolta effettuiamo prima la permutazione\n",
        "      z = self.permute(z)\n",
        "      z, _ = self.coupling_layer(z, flow_i, forward=False)\n",
        "    \n",
        "    #alla fine del for ho ottenuto un output nato dalla trasformazione che mi\n",
        "    #riporta in campioni di px\n",
        "    return z\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "64SgfiEhrTyW"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RealNVP(Scale_Net, Translation_Net, 8, p0, 64, 256)"
      ],
      "metadata": {
        "id": "-nBzsb1kh2Sy"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "_9r4x2NvhzA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.002\n",
        "#i parametri che l'optimizer deve ottimizzare sono tutti quelli del modello\n",
        "parameters_to_optimize = [p for p in model.parameters() if p.requires_grad == True]\n",
        "\n",
        "optimizer = torch.optim.Adamax(parameters_to_optimize, lr=learning_rate)"
      ],
      "metadata": {
        "id": "GNZS4Ju7lBDT"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "number_of_epochs = 1000\n",
        "\n",
        "#qui salvo il migliore modello, ossia quello che ha la loss sulla validazione migliore\n",
        "best_model = model\n",
        "best_validation_loss = 1000000\n",
        "\n",
        "patience = 0\n",
        "max_patience = 20\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "  model.train()\n",
        "  print(\"Epoca \"+str(epoch)+\" _____________________________________________________________________\")\n",
        "\n",
        "  for index_batch, batch in enumerate(training_loader):\n",
        "    #il batch ha shape 64x64. Vabbene cosi (64,64) in quanto Linear vuole questo formato\n",
        "    #ottengo la loss sul batch corrente\n",
        "    loss = model.forward(batch)\n",
        "\n",
        "    #calcolo le derivate parziali della loss rispetto ogni parametro\n",
        "    loss.backward()\n",
        "\n",
        "    #adesso ogni parametro ha in .grad il gradiente. Aggiorno il suo valore\n",
        "    optimizer.step()\n",
        "\n",
        "    #resetto il .grad di ogni parametro (altrimenti sommo quello attuale al successivo che calcoleremo nell'epoca dopo)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    print(\"   Loss: \", loss)\n",
        "\n",
        "  #alla fine di ogni epoca, valuto come si comporta la loss col validation set\n",
        "  print(\"   ___________________________\")\n",
        "  model.eval()\n",
        "  validation_loss = 0\n",
        "  N = 0\n",
        "  for index_batch, batch in enumerate(validation_loader):\n",
        "    loss_i = model.forward(batch)\n",
        "    validation_loss = validation_loss + loss_i\n",
        "    N = N +  batch.shape[0]\n",
        "\n",
        "  validation_loss = validation_loss/N\n",
        "  print(\"   Loss media validation: \"+str(validation_loss))\n",
        "\n",
        "  #se tale modello ha una loss migliore di quella attualmente migliore..\n",
        "  if validation_loss < best_validation_loss:\n",
        "    patience = 0\n",
        "    best_validation_loss = validation_loss\n",
        "    print(\"   la loss risulta essere migliore\")\n",
        "    best_model = copy.deepcopy(model)\n",
        "  else:\n",
        "    print(\"   patience= \"+ str(patience+1))\n",
        "    patience = patience + 1\n",
        "  \n",
        "  if patience > max_patience:\n",
        "    print(\"\")\n",
        "    print(\"Patience massimo superato. Fine del training\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "YfkQwlXxkTEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling"
      ],
      "metadata": {
        "id": "R8KFSxl5lg2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.sample_new_data()"
      ],
      "metadata": {
        "id": "umR4WFa-lioX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}