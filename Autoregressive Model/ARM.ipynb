{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zSA-HVRTk2g3",
        "P0R_WPsAGh1Z"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "0GDz3X_akjBz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "zSA-HVRTk2g3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abbiamo 1797 campioni, ciascuno 8x8. Ogni immagine è organizzata come un vettore di 64 pixel\n",
        "digits_dataset = load_digits()\n",
        "\n",
        "print(digits_dataset.data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYPMHwO6k3_R",
        "outputId": "ffedf10f-e0f0-4a0c-8409-fa7629a3b8a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "#ogni riga è una immagine. Vediamo un esempio\n",
        "img1 = digits_dataset.data[0]\n",
        "#facciamo il reshape del vettore di 64 elementi in 8x8\n",
        "img1 = np.reshape(img1, (8,8))\n",
        "plt.imshow(img1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xOzMXbwxl_7X",
        "outputId": "a416e7ec-e76d-4f71-f541-a727c7b860ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f2cbf031690>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#essendo ogni riga una immagine allora prendo le prime 1000 come training set, 350 come validation set e il resto come test set\n",
        "training_data = digits_dataset.data[0:1000].astype(np.float32)\n",
        "validation_data = digits_dataset.data[1000:1350].astype(np.float32)\n",
        "test_data = digits_dataset.data[1350:].astype(np.float32)"
      ],
      "metadata": {
        "id": "rH56FdMIlpbN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non alleneremo la rete dandole tutti i dati, ma batch dopo batch. Creiamo quindi un DataLoader che semplicemente dividerà i dati in batch da 64 immagini (dopo averli mischiati) e ci restituirà, quando richiesto, un batch alla volta."
      ],
      "metadata": {
        "id": "mIp49CvLncr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "QO_kV43ulD3w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "U04gfqWjp_qD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creiamo il CausalConv1D"
      ],
      "metadata": {
        "id": "Ip7patdrqBz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalConv1d(nn.Module):\n",
        "\n",
        "  def __init__(self, num_canali_input, num_canali_output, kernel_size, dilation, A=False, **kwargs):\n",
        "    super(CausalConv1d, self).__init__()\n",
        "\n",
        "    self.kernel_size = kernel_size\n",
        "    self.dilation = dilation\n",
        "    self.A = A\n",
        "\n",
        "    self.padding = (kernel_size-1)*dilation +A*1\n",
        "\n",
        "    self.conv1d = torch.nn.Conv1d(num_canali_input, num_canali_output, kernel_size, stride=1,padding=0, dilation=dilation, **kwargs)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #la funzione concatena self.padding zeri a sinistra di x\n",
        "    x = torch.nn.functional.pad(x, (self.padding, 0))\n",
        "    conv1d_out = self.conv1d(x)\n",
        "    if self.A:\n",
        "        return conv1d_out[:, :, : -1]\n",
        "    else:\n",
        "        return conv1d_out"
      ],
      "metadata": {
        "id": "eVDlZbrlqEkM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kernel = 7\n",
        "num_kernels = 256\n",
        "num_outputs = 17\n",
        "\n",
        "net = nn.Sequential(\n",
        "    CausalConv1d(num_canali_input=1, num_canali_output=num_kernels, kernel_size=kernel, dilation=1,A=True, bias=True),\n",
        "    nn.LeakyReLU(),\n",
        "    CausalConv1d(num_canali_input=num_kernels, num_canali_output=num_kernels, kernel_size=kernel,dilation=2, A=False, bias=True),\n",
        "    nn.LeakyReLU(),\n",
        "    CausalConv1d(num_canali_input=num_kernels, num_canali_output=num_kernels, kernel_size=kernel,dilation=4, A=False, bias=True),\n",
        "    nn.LeakyReLU(),\n",
        "    CausalConv1d(num_canali_input=num_kernels, num_canali_output=num_outputs, kernel_size=kernel, dilation=8, A=False, bias=True))\n"
      ],
      "metadata": {
        "id": "aPO21DdLKfxV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ARM(nn.Module):\n",
        "    def __init__(self, net, dimensione_input, num_output):\n",
        "        super(ARM, self).__init__()\n",
        "\n",
        "        self.net = net\n",
        "        self.num_output = num_output\n",
        "        self.dimensione_input = dimensione_input\n",
        "\n",
        "    def forward(self, x):\n",
        "        #faccio passare il batch attraverso la rete. Alla fine otterrò (N,17,64) con N numero immagini\n",
        "        h = self.net(x)\n",
        "        #per semplicità, voglio per ognuna delle N immagini e per ognuno dei suoi 64 pixel il vettore di probabilità\n",
        "        h = h.permute(0,2,1) # adesso ho (N,64,17)\n",
        "        #per ogni pixel converto i 17 valori in probabilità\n",
        "        p = torch.softmax(h,2)\n",
        "\n",
        "        #calcolo la MLE\n",
        "        EPS = 1.e-5\n",
        "        #serve per selezionare un valore dal vettore probabilità\n",
        "        x_one_hot = F.one_hot(x.long(), num_classes=self.num_output).squeeze()\n",
        "        #calcolo i logaritmi di ogni probabilità estratta\n",
        "        log_p = x_one_hot * torch.log(torch.clamp(p, EPS, 1. - EPS))\n",
        "        vector_of_probabilities_for_image = torch.sum(log_p, dim=-1) #contiene le 64 probabilità selezionate\n",
        "        #per ogni immagine abbiamo il vettore di probabilità. Sommiamole e cambiamo segno\n",
        "        mle_per_image = -vector_of_probabilities_for_image.sum(-1)\n",
        "        #una mle di aggregazione\n",
        "        mle_batch = mle_per_image.sum()\n",
        "\n",
        "        return mle_batch\n",
        "\n",
        "    def sample(self):\n",
        "        x_new = torch.zeros((1,self.dimensione_input))\n",
        "        #per ogni dimensione x1,x2,....,x64\n",
        "        for xi in range(self.dimensione_input):\n",
        "            #do alla rete il vettore x_new\n",
        "            h = self.net(x_new.unsqueeze(1))\n",
        "            h = h.permute(0, 2, 1)\n",
        "            #genero probabilità\n",
        "            p = torch.softmax(h, 2)\n",
        "\n",
        "            #creo una distribuzione categorica usando solo le probabilità della dimensione xi\n",
        "            x_new_xi = torch.multinomial(p[:, xi, :], num_samples=1)\n",
        "            x_new[:, xi] = x_new_xi[:,0]\n",
        "\n",
        "        return x_new"
      ],
      "metadata": {
        "id": "jccVfahvPNjm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ARM(net, dimensione_input=64, num_output=num_outputs)"
      ],
      "metadata": {
        "id": "wRJafjRfye0g"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "J-QcjrRq3O-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.002\n",
        "#i parametri che l'optimizer deve ottimizzare sono tutti quelli del modello\n",
        "parameters_to_optimize = [p for p in model.parameters() if p.requires_grad == True]\n",
        "\n",
        "optimizer = torch.optim.Adamax(parameters_to_optimize, lr=learning_rate)"
      ],
      "metadata": {
        "id": "L0IeExx03QHM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "\n",
        "number_of_epochs = 1000\n",
        "\n",
        "#qui salvo il migliore modello, ossia quello che ha la loss sulla validazione migliore\n",
        "best_model = model\n",
        "best_validation_loss = 1000000\n",
        "\n",
        "patience = 0\n",
        "max_patience = 20\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "  model.train()\n",
        "  print(\"Epoca \"+str(epoch)+\" _____________________________________________________________________\")\n",
        "\n",
        "  for index_batch, batch in enumerate(training_loader):\n",
        "    #il batch ha shape 64x64. Portiamolo in (64,1,64), ossia 64 immagini ciascuna con\n",
        "    # un canale e 64 pixel\n",
        "    batch = batch.unsqueeze(1)\n",
        "    #ottengo la loss sul batch corrente\n",
        "    loss = model.forward(batch)\n",
        "\n",
        "    #calcolo le derivate parziali della loss rispetto ogni parametro\n",
        "    loss.backward()\n",
        "\n",
        "    #adesso ogni parametro ha in .grad il gradiente. Aggiorno il suo valore\n",
        "    optimizer.step()\n",
        "\n",
        "    #resetto il .grad di ogni parametro (altrimenti sommo quello attuale al successivo che calcoleremo nell'epoca dopo)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    print(\"   Loss: \", loss)\n",
        "\n",
        "  #alla fine di ogni epoca, valuto come si comporta la loss col validation set\n",
        "  print(\"   ___________________________\")\n",
        "  model.eval()\n",
        "  validation_loss = 0\n",
        "  N = 0\n",
        "  for index_batch, batch in enumerate(validation_loader):\n",
        "    batch = batch.unsqueeze(1)\n",
        "    loss_i = model.forward(batch)\n",
        "    validation_loss = validation_loss + loss_i\n",
        "    N = N +  batch.shape[0]\n",
        "\n",
        "  validation_loss = validation_loss/N\n",
        "  print(\"   Loss media validation: \"+str(validation_loss))\n",
        "\n",
        "  #se tale modello ha una loss migliore di quella attualmente migliore..\n",
        "  if validation_loss < best_validation_loss:\n",
        "    patience = 0\n",
        "    best_validation_loss = validation_loss\n",
        "    print(\"   la loss risulta essere migliore\")\n",
        "    best_model = copy.deepcopy(model)\n",
        "  else:\n",
        "    print(\"   patience= \"+ str(patience+1))\n",
        "    patience = patience + 1\n",
        "  \n",
        "  if patience > max_patience:\n",
        "    print(\"\")\n",
        "    print(\"Patience massimo superato. Fine del training\")\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sL5fmey16Ueh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Synthetic data generation"
      ],
      "metadata": {
        "id": "uQPGiQ28utyl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creo vettore di input\n",
        "new_data = model.sample()"
      ],
      "metadata": {
        "id": "rG6mm9CtuvcO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}