{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "95Unj2pV1ELG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "YNDxmdd01P7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Abbiamo 1797 campioni, ciascuno 8x8. Ogni immagine è organizzata come un vettore di 64 pixel\n",
        "digits_dataset = load_digits()\n",
        "\n",
        "print(digits_dataset.data.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYPMHwO6k3_R",
        "outputId": "9ca1bff1-3658-4c15-8bdc-bcb8fee83435"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "#ogni riga è una immagine. Vediamo un esempio\n",
        "img1 = digits_dataset.data[0]\n",
        "#facciamo il reshape del vettore di 64 elementi in 8x8\n",
        "img1 = np.reshape(img1, (8,8))\n",
        "plt.imshow(img1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "xOzMXbwxl_7X",
        "outputId": "1d863f58-640f-4af6-c3db-2272299d583c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f5340c8b6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#essendo ogni riga una immagine allora prendo le prime 1000 come training set, 350 come validation set e il resto come test set\n",
        "training_data = digits_dataset.data[0:1000].astype(np.float32)\n",
        "validation_data = digits_dataset.data[1000:1350].astype(np.float32)\n",
        "test_data = digits_dataset.data[1350:].astype(np.float32)"
      ],
      "metadata": {
        "id": "rH56FdMIlpbN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non alleneremo la rete dandole tutti i dati, ma batch dopo batch. Creiamo quindi un DataLoader che semplicemente dividerà i dati in batch da 64 immagini (dopo averli mischiati) e ci restituirà, quando richiesto, un batch alla volta."
      ],
      "metadata": {
        "id": "mIp49CvLncr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_loader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "validation_loader = DataLoader(validation_data, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ],
      "metadata": {
        "id": "QO_kV43ulD3w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "XR07Litx1eVK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedConvolutionalLayer(nn.Module):\n",
        "   def __init__(self,A,num_canali_input,num_filters, kernel_size,padding=\"same\",**kwargs):\n",
        "    super(MaskedConvolutionalLayer, self).__init__()\n",
        "    \n",
        "    #se siamo nel primo layer (A=true) o meno\n",
        "    self.A = A\n",
        "    \n",
        "    #creo un classico Conv2D con padding=\"same\", stride=1 e filtri num_filters\n",
        "    self.conv2d = nn.Conv2d(in_channels=num_canali_input, out_channels=num_filters,kernel_size=kernel_size,padding=padding)\n",
        "    \n",
        "    #Maschero il filtro \n",
        "    #1. Ottengo la shape (N,N) del kernel\n",
        "    kernel_shape = self.conv2d.kernel_size\n",
        "    #2. Creo una maschera (N,N) di zeri\n",
        "    mask = torch.zeros(kernel_shape)\n",
        "    #Inserisco 1 dalla prima riga fino a metà del kernel (divisione intera)\n",
        "    mask[: kernel_shape[0] // 2]=1.0\n",
        "    #Inserisco 1 dalla prima colonna fino a metà. Se è di tipo A non considero\n",
        "    #il centro, altrimenti lo includo\n",
        "    mask[kernel_shape[0] // 2,: kernel_shape[1] // 2 + (not A) ]=1.0\n",
        "\n",
        "    #modifico il kernel\n",
        "    self.conv2d.weight.data = self.conv2d.weight.data * mask\n",
        "   \n",
        "   def forward(self,x):\n",
        "      return self.conv2d(x)\n",
        "    "
      ],
      "metadata": {
        "id": "mpVoJq271ffJ"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self,num_canali_input,number_of_filters,**kwargs):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=num_canali_input,out_channels=number_of_filters//2,kernel_size=1)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "    self.masked_conv = MaskedConvolutionalLayer(False,num_canali_input=number_of_filters//2,num_filters=number_of_filters//2, kernel_size=3)\n",
        "    self.conv2 =nn.Conv2d(in_channels=number_of_filters//2,out_channels=number_of_filters,kernel_size=1)\n",
        "\n",
        "  def forward(self,input):\n",
        "    \n",
        "    x = self.conv1(input)\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    x = self.masked_conv(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "\n",
        "    #implemento la skip connection semplicemente sommando l'input all'output x\n",
        "    return x + input\n"
      ],
      "metadata": {
        "id": "QP4qz7HLOH-P"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelCNN(nn.Module):\n",
        "  def __init__(self,num_canali_input,num_canali_output,**kwargs):\n",
        "    super(PixelCNN, self).__init__()\n",
        "    \n",
        "    self.num_canali_input = num_canali_input\n",
        "    self.num_canali_output = num_canali_output\n",
        "\n",
        "    #il primo layer di convoluzione non deve considerare il pixel centrale\n",
        "    self.masked_conv_A = MaskedConvolutionalLayer(True,num_canali_input,num_filters=128, kernel_size=7)\n",
        "\n",
        "    #gli ultimi due layer di convoluzione non deve considerare il pixel centrale e deve avere kernel 1 e padding=\"valid\"\n",
        "    self.masked_conv_B = MaskedConvolutionalLayer(False,num_canali_input=128,num_filters=128, kernel_size=1, padding=\"valid\")\n",
        "    \n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    #residual block\n",
        "    self.residual_block = ResidualBlock(num_canali_input=128,number_of_filters=128)\n",
        "\n",
        "    #output. Il kernel deve avere 17 output rappresentati le 17 probabilità per i 17 valori (per ogni pixel)\n",
        "    self.output_layer = nn.Conv2d(in_channels=128,out_channels=num_canali_output, kernel_size=1, padding=\"valid\")\n",
        "\n",
        "  def forward(self, input, mode=\"training\"):\n",
        "    x = self.masked_conv_A(input)\n",
        "\n",
        "    x = self.relu(x)\n",
        "\n",
        "    for i in range(5):\n",
        "      x = self.residual_block(x)\n",
        "    \n",
        "    for i in range(2):\n",
        "      x = self.masked_conv_B(x)\n",
        "    \n",
        "    output = self.output_layer(x)\n",
        "    output = output.permute(0,2,3,1)\n",
        "\n",
        "    output = torch.softmax(output, dim=3)\n",
        "\n",
        "    if mode==\"prediction\":\n",
        "      return output\n",
        "\n",
        "    EPS = 1.e-5\n",
        "    x_one_hot = F.one_hot(input.long(), num_classes=self.num_canali_output) .squeeze()\n",
        "    log_p = x_one_hot * torch.log(torch.clamp(output, EPS, 1. - EPS))\n",
        "    vector_of_probabilities_per_image = torch.sum(log_p, dim=-1) \n",
        "    mle_per_image = -vector_of_probabilities_per_image.sum(dim=(1,2))\n",
        "    mle_loss_batch = mle_per_image.mean()\n",
        "\n",
        "    return mle_loss_batch\n"
      ],
      "metadata": {
        "id": "b1uPzSRzMoem"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PixelCNN(1,17)"
      ],
      "metadata": {
        "id": "0bJvmqALkDb3"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "AK-4UVOukR1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.002\n",
        "#i parametri che l'optimizer deve ottimizzare sono tutti quelli del modello\n",
        "parameters_to_optimize = [p for p in model.parameters() if p.requires_grad == True]\n",
        "\n",
        "optimizer = torch.optim.Adamax(parameters_to_optimize, lr=learning_rate)"
      ],
      "metadata": {
        "id": "GNZS4Ju7lBDT"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy \n",
        "\n",
        "best_validation_loss = 1000000\n",
        "\n",
        "patience = 0\n",
        "max_patience = 20\n",
        "number_of_epochs = 1000\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "  model.train()\n",
        "  print(\"Epoca \"+str(epoch)+\" _____________________________________________________________________\")\n",
        "\n",
        "  for index_batch, batch in enumerate(training_loader):\n",
        "    #il batch ha shape 64x64. Portiamolo in (64,1,64), ossia 64 immagini ciascuna con\n",
        "    # un canale e 64 pixel\n",
        "    batch = batch.unsqueeze(1)\n",
        "    #stavolta manipoliamo immagini bidimensionali invece che \"immagini flattate\"\n",
        "    batch = batch.reshape(batch.size(0), batch.size(1),8,8)\n",
        "    #ottengo la loss sul batch corrente\n",
        "    loss = model.forward(batch)\n",
        "\n",
        "    #calcolo le derivate parziali della loss rispetto ogni parametro\n",
        "    loss.backward()\n",
        "\n",
        "    #adesso ogni parametro ha in .grad il gradiente. Aggiorno il suo valore\n",
        "    optimizer.step()\n",
        "\n",
        "    #resetto il .grad di ogni parametro (altrimenti sommo quello attuale al successivo che calcoleremo nell'epoca dopo)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    print(\"   Loss: \", loss)\n",
        "\n",
        "  #alla fine di ogni epoca, valuto come si comporta la loss col validation set\n",
        "  print(\"   ___________________________\")\n",
        "  model.eval()\n",
        "  validation_loss = 0\n",
        "  N = 0\n",
        "  for index_batch, batch in enumerate(validation_loader):\n",
        "    batch = batch.unsqueeze(1)\n",
        "    batch = batch.reshape(batch.size(0), batch.size(1),8,8)\n",
        "    loss_i = model.forward(batch)\n",
        "    validation_loss = validation_loss + loss_i\n",
        "    N = N +  1\n",
        "\n",
        "  validation_loss = validation_loss/N\n",
        "  print(\"   Loss media validation: \"+str(validation_loss))\n",
        "\n",
        "  #se tale modello ha una loss migliore di quella attualmente migliore..\n",
        "  if validation_loss < best_validation_loss:\n",
        "    patience = 0\n",
        "    best_validation_loss = validation_loss\n",
        "    print(\"   la loss risulta essere migliore\")\n",
        "    #best_model = copy.deepcopy(model)\n",
        "  else:\n",
        "    print(\"   patience= \"+ str(patience+1))\n",
        "    patience = patience + 1\n",
        "  \n",
        "  if patience > max_patience:\n",
        "    print(\"\")\n",
        "    print(\"Patience massimo superato. Fine del training\")\n",
        "    break\n"
      ],
      "metadata": {
        "id": "YfkQwlXxkTEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sampling"
      ],
      "metadata": {
        "id": "1b7DJQS5yo8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate():\n",
        "  generated_image = torch.zeros((1,1,8,8))\n",
        "  batch,channels, rows, cols = generated_image.shape\n",
        "  for row in range(rows):\n",
        "    for col in range(cols):\n",
        "      for channel in range(channels): #utile per rgb\n",
        "        #calcolo i vettori di probabilità ma prendo solo quel vettore (di 17)\n",
        "        #del pixel (row,col)\n",
        "        probs = model.forward(generated_image, mode=\"prediction\")[:, row, col, :]\n",
        "        pixel_value = torch.multinomial(probs, num_samples=1)\n",
        "        generated_image[:, row, col, channel] = pixel_value\n",
        "    return generated_image\n",
        "\n",
        "def generete_image():\n",
        "  generated_image = generate()\n",
        "  return generated_image\n"
      ],
      "metadata": {
        "id": "1fMefrpKyqof"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generete_image()"
      ],
      "metadata": {
        "id": "jpWjT_FSza-_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}